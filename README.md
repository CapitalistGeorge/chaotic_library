# ðŸ§  Reservoir Neural Networks with Fourier Layer
### Advanced Reservoir Computing Techniques for Chaotic Time Series Prediction
**By [ACS Lab, ITMO University](https://iai.itmo.ru/)** Â· 2025

Official repository for the paper:
> **A. Kovantsev, R. Vysotskiy (2025). _Advanced Reservoir Neural Network Techniques for Chaotic Time Series Prediction._ SSRN 5481760.**

---

## ðŸ“˜ Overview

We present **ESNâ€‘F** â€” the **Echo State Network (ESN)** enhanced with **Fourier features** and **polynomial expansion** for forecasting **chaotic / nonlinear** time series. The approach keeps the **reservoir untrained** and learns only a **ridge readout**, while enriching inputs with **periodic (sin/cos)** and **nonlinear** bases that improve longâ€‘horizon stability.

Use cases include **finance/economics**, **risk modeling**, and other **nonâ€‘stationary** domains.

---

## Contents

* [Why this project?](#why-this-project)
* [Install](#install)
* [Quickstart](#quickstart)
* [Generative (multi-step) forecasting](#generative-multi-step-forecasting)
* [Multivariate example](#multivariate-example)
* [Background & Theory](#background--theory)

  * [Echo State Network (leaky ESN)](#echo-state-network-leaky-esn)
  * [Fourier & Polynomial feature blocks (FAN)](#fourier--polynomial-feature-blocks-fan)
  * [Ridge readout & objective](#ridge-readout--objective)
  * [Forecasting strategies](#forecasting-strategies)
  * [Why it helps with chaos & nonâ€‘stationarity](#why-it-helps-with-chaos--non-stationarity)
* [Predictability metrics (optional)](#predictability-metrics-optional)
* [ðŸ“ Predictability features (formulas)](#-predictability-features-formulas)
* [Hyperparameters](#hyperparameters)
* [Complexity & scaling](#complexity--scaling)
* [Reproducibility](#reproducibility)
* [Figures (placeholders)](#figures-placeholders)
* [ðŸ“Š Experimental Results (from the paper)](#-experimental-results-from-the-paper)
* [Project layout](#project-layout)
* [Contributing](#contributing)
* [License & citation](#license--citation)
* [Troubleshooting / FAQ](#troubleshooting--faq)

---

## Why this project?

Longâ€‘horizon forecasting on chaotic / weakly stationary signals is tricky: standard RNNs tend to drift, while purely statistical baselines miss nonlinear structure. **EnhancedESN_FAN** keeps a **random, untrained reservoir** for rich dynamics and augments the readout with **deterministic Fourier harmonics** and **polynomial features**. The linear readout is trained via **ridge regression**, keeping training fast, convex, and robust.

---

## Install

```bash
# From source (recommended for now)
git clone https://github.com/CapitalistGeorge/chaotic_library.git
cd chaotic_library

python -m venv .venv
# macOS/Linux
source .venv/bin/activate
# Windows (PowerShell)
# .venv\Scripts\Activate.ps1

python -m pip install -U pip

# Option 1: install dependencies only (for running notebooks / scripts)
pip install -r requirements.txt

# Option 2: install the package in editable mode (preferred while developing)
pip install -e .
```

*Planned:* after the API is frozen weâ€™ll publish to PyPI, so you can `pip install esn-fan` (package name TBD). The import path in the examples below assumes a module `enhanced_esn_fan.py` at the project root; adjust if packaged differently.

---

## Quickstart

```python
import numpy as np
from chaotic_library import EnhancedESN_FAN  # adjust import path if packaged differently

# 1) Build training data (shape: [n_timesteps, input_dim])
# Univariate example: input_dim=1 â†’ X is 2D with one column, y is 1D
T = 1200
noise = 0.1 * np.random.randn(T)
signal = np.sin(2*np.pi*np.arange(T)/50) + 0.25*np.sin(2*np.pi*np.arange(T)/7) + noise

X = signal[:-1].reshape(-1, 1)   # features are previous value(s)
y = signal[1:]                   # next-step target

# 2) Initialize Enhanced ESN + FAN features (Fourier + polynomial)
esn = EnhancedESN_FAN(
    input_dim=1,          # number of input features per timestep (columns of X)
    reservoir_size=800,
    spectral_radius=0.95,
    sparsity=0.1,
    ridge_alpha=1e-2,
    leaking_rate=0.3,
    poly_order=2,
    fan_terms=8,
    random_state=42,
    clip_value=3.0,
)

# 3) Fit and one-step-ahead predictions (teacher forcing / open loop)
esn.fit(X, y)
y_hat = esn.predict(X[:100])        # shape: (100,) for univariate target
print("y_hat shape:", np.asarray(y_hat).shape)
```

---

## Generative (multi-step) forecasting

```python
# Seed with the last observed input row (shape: [1, input_dim])
seed = X[-1:].copy()

# Produce next 300 steps autoregressively
future = esn.predict(seed, generative_steps=300)   # shape: (300,) for univariate

# Concatenate history + forecast for plotting
full = np.concatenate([signal, future.ravel()])
```

---

## Multivariate example

```python
import numpy as np
from chaotic_library import EnhancedESN_FAN

# Suppose you have 3 exogenous drivers + the main signal â†’ input_dim=4
n = 2000
main = np.sin(2*np.pi*np.arange(n)/30) + 0.05*np.random.randn(n)
x1 = np.cos(2*np.pi*np.arange(n)/100)
x2 = np.sin(2*np.pi*np.arange(n)/7)
x3 = 0.01*np.arange(n)  # slow trend proxy

# Features: use current drivers + lagged main as input; predict next main
X = np.column_stack([main[:-1], x1[:-1], x2[:-1], x3[:-1]])  # shape (n-1, 4)
y = main[1:]                                               # shape (n-1,)

model = EnhancedESN_FAN(
    input_dim=4,
    reservoir_size=500,
    spectral_radius=0.9,
    sparsity=0.1,
    ridge_alpha=0.1,
    leaking_rate=0.4,
    poly_order=2,
    fan_terms=6,
    random_state=7,
)

model.fit(X, y)
pred = model.predict(X[:128])              # one-step predictions (teacher forcing)
gen  = model.predict(X[-1:], generative_steps=200)  # recursive forecast
```

---

## Background & Theory

### Echo State Network (leaky ESN)

Reservoir state \(\mathbf{x}_t \in \mathbb{R}^{N_r}\) evolves under a fixed random dynamical system:

$$
\begin{aligned}
\tilde{\mathbf{x}}_t &= \tanh\left( \mathbf{W} \mathbf{x}_{t-1} + \mathbf{W}_{\text{in}} [1; u_t] \right), \\
\mathbf{x}_t &= (1-\alpha) \mathbf{x}_{t-1} + \alpha \tilde{\mathbf{x}}_t,
\end{aligned}
$$

where:
- \(u_t\) is the input (e.g., components of \(X_t\))
- \([1; u_t]\) denotes a biasâ€‘augmented input
- \(\alpha\) is the **leaking rate** (`leaking_rate` parameter)

To satisfy the **echo state property** (state forgets initial conditions), scale the reservoir so that its spectral radius \(\rho(\mathbf{W})\) is near 1 (practically 0.7â€“1.2 with leakage; `spectral_radius` parameter).

We collect features at time \(t\) by concatenating the reservoir state with deterministic blocks:

$$
\mathbf{z}_t = \big[ \mathbf{x}_t \mid \phi_{\text{poly}}(u_t) \mid \phi_{\text{Fourier}}(u_t) \big] \in \mathbb{R}^{D}
$$

**Notation:**
- \(\mathbf{W}\): reservoir weight matrix
- \(\mathbf{W}_{\text{in}}\): input weight matrix  
- \(\phi_{\text{poly}}\): polynomial features
- \(\phi_{\text{Fourier}}\): Fourier features
- \(D\): total feature dimension

### Fourier & Polynomial feature blocks (FAN)

* **PolynomialFeatures** of degree (d) (=`poly_order`): ([u_t, u_t^2, \dots, u_t^d]) per input dimension (no extra bias term; bias provided separately).
* **Fourier (FAN) features** with harmonics (k=1..K) (=`fan_terms`): for each input dimension, compute (\sin(2\pi k X)) and (\cos(2\pi k X)). These inject periodic structure explicitly, so the reservoir does not have to "discover" it from scratch.

### Ridge readout & objective

Only the final linear readout (\mathbf{W}*{\text{out}} \in \mathbb{R}^{D\times m}) is trained via ridge regression:
[
\min*{\mathbf{W}*{\text{out}}}; \big|\mathbf{Y} - \mathbf{Z}\mathbf{W}*{\text{out}}\big|*2^2
;+; \lambda \big|\mathbf{W}*{\text{out}}\big|*2^2,
\quad\Rightarrow\quad
\mathbf{W}*{\text{out}} = (\mathbf{Z}^\top\mathbf{Z} + \lambda \mathbf{I})^{-1}\mathbf{Z}^\top\mathbf{Y}.
]
Columns of (\mathbf{Z}) should be standardized for numerical stability (the implementation uses `StandardScaler`).

### Forecasting strategies

* **Teacher forcing / open loop (default in `predict(X)`):** oneâ€‘step predictions using the provided inputs.
* **Generative / recursive (`predict(seed, generative_steps=m)`):** feed back model outputs as inputs to generate future steps.
* **Hybrid (future option):** recursive core with direct corrections for selected horizons.

### Why it helps with chaos & nonâ€‘stationarity

* Reservoir provides a **rich, fading memory** of nonlinear histories.
* Fourier layer **anchors** periodic structure â†’ less burden on the reservoir.
* Polynomial bias **stabilizes** local trends and offsets.
* Ridge readout **tames variance** and keeps training convex & fast.

---

## Predictability metrics (optional)

You can compute these to **cluster series by predictability** and adapt hyperparameters:

* **Hurst exponent (H)** â€” persistence (>0.5) vs. antiâ€‘persistence (<0.5)
* **Correlation dimension (Dâ‚‚)** â€” attractor dimension (Grassbergerâ€“Procaccia)
* **Max Lyapunov exponent (Î»â‚˜â‚â‚“)** â€” sensitivity to initial conditions
* **Kolmogorovâ€“Sinai entropy (KSE)** â€” information production rate
* **# Prevailing harmonics** â€” count strong spectral peaks (e.g., via periodogram)

Use the cluster to pick `reservoir_size`, `spectral_radius`, and `fan_terms`. For highly chaotic signals (large Î»â‚˜â‚â‚“), prefer slightly lower `spectral_radius` and stronger regularization (`ridge_alpha`).

---

## ðŸ“ Predictability features (formulas)

NOTATION:
  xÌ„_Ï„    - sample mean on window of length Ï„
  Î¸(Â·)   - Heaviside step function
  Ï(i,j) - distance in reconstructed phase space (delay embedding optional)
  x'_i   = x_i - x_{i-1}  (first difference)

HURST EXPONENT
H = ln( R(Ï„) / S(Ï„) ) / ln(Î±Â·Ï„)                                 (6)

where:
R(Ï„) = max[1â‰¤tâ‰¤Ï„] [ Î£[i=1 to t] (x_i - xÌ„_Ï„) ] 
       - min[1â‰¤tâ‰¤Ï„] [ Î£[i=1 to t] (x_i - xÌ„_Ï„) ]                 (7)
       
S(Ï„) = âˆš[ (1/Ï„) Â· Î£[t=1 to Ï„] (x_t - xÌ„_Ï„)Â² ]                   (8)

Note: in classical R/S analysis, H is the slope of ln(R/S) vs ln Ï„ 
(i.e., Î±=1). Including a constant Î± is equivalent up to offset.

KOLMOGOROV-SINAI ENTROPY (KSE)
Definition via entropy-rate upper bound:

h_Î¼(T,Î¾) = - lim[nâ†’âˆž] (1/n) 
           Ã— Î£[iâ‚,...,iâ‚™] Î¼( Tâ»Â¹C_iâ‚ âˆ© ... âˆ© Tâ»â¿C_iâ‚™ ) Â· ln Î¼(...)  (9)

h_Î¼^KS(T) = sup[Î¾] h_Î¼(T,Î¾)                                       (10)

CORRELATION DIMENSION
dâ‚‚ = lim[râ†’0] lim[mâ†’âˆž] ln C(r) / ln r                           (11)

where:
C(r) = 1 / [m(m-1)] 
       Ã— Î£[i=1 to m] Î£[j=i+1 to m] Î¸( r - Ï(i,j) )              (12,13)

---

## Hyperparameters

| Parameter         | Meaning                                             | Typical range / tips                |
| ----------------- | --------------------------------------------------- | ----------------------------------- |
| `reservoir_size`  | Number of reservoir units                           | 300â€“2000                            |
| `spectral_radius` | Spectral radius after scaling                       | 0.7â€“1.2 with leakage                |
| `sparsity`        | Fraction of **zeroed** connections (mask threshold) | 0.7â€“0.95 for very sparse reservoirs |
| `leaking_rate`    | Leaky integrator rate                               | 0.1â€“0.5 for longer memory           |
| `ridge_alpha`     | Ridge regularization strength                       | 1eâˆ’6â€“1e0                            |
| `poly_order`      | Polynomial degree (no bias term)                    | 1â€“3                                 |
| `fan_terms`       | #Fourier harmonics per input dimension              | 3â€“12                                |
| `clip_value`      | Clip for scaled inputs in generative mode           | 2â€“5                                 |
| `random_state`    | Seed                                                | set for reproducibility             |

---

## Complexity & scaling

* **State update:** (O(T,N_r,s)) with sparsity fraction (s) (dense â†’ (O(T,N_r^2))).
* **Readout training:** build (\mathbf{Z}\in\mathbb{R}^{T\times D}); solve ridge via Cholesky/QR: ~(O(D^3)) (usually (D \ll T)).
* **Memory:** (O(TD)) if keeping all features; use chunked/online solvers for very long series.

---

## Reproducibility

* Fix `random_state` for weights and reservoirs.
* Standardize inputs and feature matrix consistently across train/forecast.
* Log: hyperparameters, seeds, and package versions.
* Provide notebooks that mirror experiments and regenerate figures.

---

## Figures (placeholders)

Put images in `docs/figures/` (SVG/PNG). Filenames below are suggestions; feel free to rename.

**Model architecture** <img src="docs/figures/fig-architecture-esnf.svg" width="760" alt="ESN-FAN architecture: input â†’ reservoir (leaky ESN) â†’ concat Fourier & poly â†’ ridge readout"/>

**Reservoir dynamics (phase portrait)** <img src="docs/figures/fig-reservoir-dynamics.png" width="760" alt="Reservoir state trajectories and fading memory with different leak rates"/>

**Ablation: effect of feature blocks** <img src="docs/figures/fig-ablation-fourier-poly.png" width="760" alt="MAPE/RMSE across ESN, ESN+Poly, ESN+Fourier, ESN+Fourier+Poly"/>

**Predictability clustering** <img src="docs/figures/fig-predictability-clusters.svg" width="760" alt="Series clustered by H, D2, Î»max, KSE, #harmonics; hyperparameter recipes per cluster"/>

**Long-horizon forecast vs. truth** <img src="docs/figures/fig-forecast-long-horizon.png" width="760" alt="Ground truth vs ESN-FAN forecast with prediction intervals; error growth comparison"/>

*(If you prefer pure Markdown images, you can also use: `![Architecture](docs/figures/fig-architecture-esnf.svg)` and similar.)*

---

## ðŸ“Š Experimental Results (from the paper)

### M4 (clustered by predictability, MAPE % â†“)

| Cluster |    ESNâ€‘F |      ESN | LGBM | Prophet |   SSA |
| ------- | -------: | -------: | ---: | ------: | ----: |
| Good    | **3.44** |     3.56 | 3.72 |    6.86 | 18.03 |
| Bad     |     5.26 | **5.19** | 5.39 |    8.57 | 20.05 |

In the **Bad** cluster, ESNâ€‘F beats LGBM by â‰¥1 pp in **27%** of series (LGBM better in **15%**; remainder negligible).

### Moscow Real Estate (weekly, MAPE % â†“)

| Model     |     MAPE |
| --------- | -------: |
| **ESNâ€‘F** | **2.56** |
| ESN       |     3.19 |
| LGBM      |     7.18 |

Chaotic traits of the realâ€‘estate series (for interpretation): Hurst **0.65**, Noise **0.99**, Corr. dimension **1.33**, max Lyapunov **0.01**, **KSE** **1.84**, (N_{Fh}=30).

---

## Project layout

```text
.
â”œâ”€â”€ src/
â”‚   â””â”€â”€ chaotic_library/
â”‚       â”œâ”€â”€ __init__.py            # public API (EnhancedESN_FAN, chaos measures, version, etc.)
â”‚       â”œâ”€â”€ enhanced_esn_fan.py    # ESN-FAN implementation
â”‚       â””â”€â”€ chaotic_measures.py    # Hurst, Lyapunov, entropy, dimensionality, etc.
â”œâ”€â”€ tests/                         # unit tests
â”œâ”€â”€ .github/workflows/             # CI (linting, tests)
â”œâ”€â”€ requirements.txt               # runtime/dev dependencies
â”œâ”€â”€ pyproject.toml                 # packaging metadata (build system, project info)
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## Contributing

* Run linters/formatters before committing (e.g., `ruff check .` / `ruff format .`).
* Add/extend tests in `tests/`.
* For new feature blocks, include a minimal notebook demo.
* Keep figures reproducible from notebooks where possible.

---

## License & citation

**License:** MIT â€” see [`LICENSE`](./LICENSE).

**Citation (placeholder):** If you use this repository, please cite the corresponding preprint/paper.

```
@misc{esn_fan_2025,
  title   = {Enhanced Echo State Network with Fourier Analysis Network (FAN) Features},
  author  = {Kovantsev, A. and Vysotskiy, R.},
  year    = {2025},
  note    = {preprint},
  howpublished = {URL: add when available}
}
```

---

## Troubleshooting / FAQ

**Q: My recursive (generative) forecast saturates or explodes.**
A: Increase `ridge_alpha`, decrease `spectral_radius`, and consider a slightly larger `clip_value` (2â€“5). Also try lowering `leaking_rate` for longer memory.

**Q: Shapes?**
A: `X` must be 2D: `(n_timesteps, input_dim)`. For univariate, reshape with `reshape(-1, 1)`. `y` can be 1D (univariate) or 2D (multiâ€‘output).

**Q: Scaling consistency between train and predict?**
A: The model uses internal scalers. Ensure that polynomial and Fourier features at prediction time are computed in a way consistent with training. If you modify the code, apply the same input scaling before feature generation in all paths (teacher forcing and generative).
